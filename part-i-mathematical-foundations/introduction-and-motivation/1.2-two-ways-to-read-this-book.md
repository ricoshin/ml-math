---
description: 1.2 이 책을 읽는 두 가지 방법 (p.13)
---

# 1.2 Two Ways to Read This Book

* 머신러닝을 이해하는 두가지 전략
  * **Bottom-up**:
    * 기초부터 심화로
    * 수학과 같은 기술적 분야에서 선호
    * 장점: 항상 이미 배웠던 개념에 의지할 수 있음
    * 단점: 이걸 언제 어떻게 써먹을지 몰라 지루함
      * 흥미 뿐 아니라 동기 부족으로 쉽게 잊어버림
  * **Top-down**:
    * 실용적 필요에서 기본적 요구사항으로
    * 목표지향적 접근법
    * 장점: 항상 해당 개념을 왜 써야하는지 무엇을 배워야하는지 알고 있음
    * 단점: 기초가 탄탄하지 않아 이해할 수 없는 단어들을 기억해야 함
* 이 책은 위의 두가지 전략을 모두 사용하여 읽을 수 있음
  * Part I은 Part II의 기반이 되는 내용이지만, 스킵하고 Part II를 먼저보고 필요할때 돌아와도 됨
  * Part II는 많은 부분 서로 독립적인 내용이므로, 순서에 관계없이 봐도 됨
    * 단, 전반적으로 난이도는 오름차순
  * 이를 도와주기 위해, 책 내용에 두 파트를 연결해주는 포인터들이 포함되어 있음



* 이 책의 내용은 기초적 개념\(Part I\)과 응용 사례\(Part II\)가 분리되어 있음
  * Part I: 기초개념
    * Chapter 1: **Introduction and Motivation**
      * 현재 챕터
    * Chapter 2: **Linear Algebra**
      * 벡터와 행렬에 대한 학습
        * 벡터\(vector\): 수치적 데이터 / 행렬\(matrix\): 그 데이터의 집합\(table/ collection\)
    * Chapter 3: **Analytic Geometry**
      * 벡터간 유사도\(similarity\)와 거리\(distance\)에 관련된 내용
      * 벡터 간 유사도\(similarity\):
        * 두 개의 벡터는 실제 세계에 있는 두 개의 대상을 나타냄
        * 비슷한 벡터는 비슷한 예측값을 가질 것으로 기대할 수 있어야 함
        * 두 개의 벡터를 입력으로 받아서 유사도를 나타내는 수치값을 반환하는 연산이 필요
    * Chapter 4: **Matrix Decomposition**
      * 어떤 행렬 연산은 엄청나게 유용: 데이터의 직관적 해석 및 효율적 학습에 기여 
    * Chapter 5: **Vector Calculus**
      * gradient의 개념을 설명 
    * Chapter 6: **Probability & Distributions**
      * 확률적 표현, 불확실성에 대한 수량화가 필요한 경우
        * 데이터에 들어있는 노이즈를 표현하고 싶을때
        * 예측의 불확실성\(uncertainty\)을 표현하고 싶을때
    * Chapter 7: **Optimization**
      * 함수의 최대 최소를 찾는 방법
      * Chapter 5에 배운 gradient를 이용하여 해를 찾을 수 있는 방향을 탐색
  * Part II: 응용사례
    * Chapter 8: **When Models Meet Data**
      * 머신러닝의 세가지 컴포넌트에 대해 수학적인 방법으로 다시 알아봄
        * 세가지 컴포넌트: data, model, learning
      * 머신러닝 시스템을 제대로 평가할 수 있는 실험 구성에 대한 가이드라인
        * Generalization\(unseen data에 대해 잘하기\)에 대한 관점
    * Chapter 9: **Regression**
      * 목적: 다음 함수를 찾는 것$$f: \mathbf{x} \rightarrow y $$ 
        * $$\mathbf{x} \in \mathbb{R}^{D}$$: 다차원 벡터 \(입력\) / $$y\in \mathbb{R}$$: **실수** 레이블 \(출력\)
      * 방법: 전통적인 두가지 모델 피팅\(parameter estimation; 모수 추정\) 방법
        * MLE\(Maximum Likelihood Estimation\)
        * MAP\(Maximum A Posteriori\)
          * Bayesian linear regression: 파라미터를 최적화하는 대신 'integrate out'
          * \(editor\) integrate out은 marginalize out을 말하는 건가?
      * 실수 레이블 존재 
    * Chapter 10: **Dimensionality Reduction**
      * 목적: 고차원의 데이터를 더 compact한 저차원의 representation으로 내타내는 것
        * 더 분석이 쉬움
      * 방법: PCA\(Principle Component Analysis\)를 다룸
      * 레이블이 존재하지 않음 \(데이터를 모델링하는데만 관심\)
    * Chapter 11: **Density Estimation**
      * 목적: 주어진 데이터셋을 설명하는 확률 분포를 찾는 것
      * 방법: GMM\(Gaussian Mixture Model\)
      * 레이블이 존재하지 않음 
    * Chapter 12: **Classification**
      * 목적: 다음 함수를 찾는 것$$f: \mathbf{x} \rightarrow y $$ 
        * $$\mathbf{x} \in \mathbb{R}^{D}$$: 다차원 벡터 \(입력\) / $$y\in \mathbb{R}$$: **정수**\(각 클래스 대응\) 레이블 \(출력\)
      * 방법: SVM\(Support Vector Machine\)
      * 정수 레이블 존재 

![&#xAC01; &#xCC55;&#xD130;&#xC758; &#xAC1C;&#xB150;&#xB4E4;&#xC774; &#xC5B4;&#xB5BB;&#xAC8C; &#xC5F0;&#xACB0;&#xB418;&#xACE0; &#xC788;&#xB294;&#xC9C0; &#xBCF4;&#xC5EC;&#xC8FC;&#xB294; &#xADF8;&#xB9BC;](../../.gitbook/assets/image%20%2850%29.png)

